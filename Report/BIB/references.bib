@article{active learning thesis,
url = {http://dx.doi.org/10.1561/2200000037},
year = {2014},
volume = {7},
journal = {Foundations and Trends® in Machine Learning},
title = {Theory of Disagreement-Based Active Learning},
doi = {10.1561/2200000037},
issn = {1935-8237},
number = {2-3},
pages = {131-309},
author = {Steve Hanneke}
}

@techreport{literature survey,
	Author = {B. Settles},
	Institution = {University of Wisconsin--Madison},
	Number = {1648},
	Title = {Active Learning Literature Survey},
	Type = {Computer Sciences Technical Report},
	Year = {2009}}

@article{robust interactive learning,
  author    = {Maria{-}Florina Balcan and
               Steve Hanneke},
  title     = {Robust Interactive Learning},
  journal   = {CoRR},
  volume    = {abs/1111.1422},
  year      = {2011},
  url       = {http://arxiv.org/abs/1111.1422},
  timestamp = {Mon, 05 Dec 2011 18:04:41 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1111-1422},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{agnostic,
title = "Agnostic active learning ",
journal = "Journal of Computer and System Sciences ",
volume = "75",
number = "1",
pages = "78 - 89",
year = "2009",
note = "Learning Theory 2006 ",
issn = "0022-0000",
doi = "http://dx.doi.org/10.1016/j.jcss.2008.07.003",
url = "http://www.sciencedirect.com/science/article/pii/S0022000008000652",
author = "Maria-Florina Balcan and Alina Beygelzimer and John Langford",
keywords = "Active learning",
keywords = "Agnostic setting",
keywords = "Sample complexity",
keywords = "Linear separators "
}

@Article{true sample complexity,
author="Balcan, Maria-Florina
and Hanneke, Steve
and Vaughan, Jennifer Wortman",
title="The true sample complexity of active learning",
journal="Machine Learning",
year="2010",
volume="80",
number="2",
pages="111--139",
abstract="We describe and explore a new perspective on the sample complexity of active learning. In many situations where it was generally believed that active learning does not help, we show that active learning does help in the limit, often with exponential improvements in sample complexity. This contrasts with the traditional analysis of active learning problems such as non-homogeneous linear separators or depth-limited decision trees, in which $\Omega$(1/$\epsilon$) lower bounds are common. Such lower bounds should be interpreted carefully; indeed, we prove that it is always possible to learn an $\epsilon$-good classifier with a number of samples asymptotically smaller than this. These new insights arise from a subtle variation on the traditional definition of sample complexity, not previously recognized in the active learning literature.",
issn="1573-0565",
doi="10.1007/s10994-010-5174-y",
url="http://dx.doi.org/10.1007/s10994-010-5174-y"
}


